CHECKPOINT_DIR: "/snel/share/joel/transformer_modeling/"
DATA:
  DATAPATH: "../data"
  TRAIN_FILENAME: 'lfads_lorenz.h5'
  VAL_FILENAME: 'lfads_lorenz.h5'
  LOG_EPSILON: .0000001
  OVERFIT_TEST: False
MODEL:
  TRIAL_LENGTH: 50
  LEARNABLE_POSITION: True
  PRE_NORM: True
  FIXUP_INIT: True
  EMBED_DIM: 2 # We embed to 2 here so transformer can use 2 heads. Perf diff is minimal.
  LOGRATE: True
TRAIN:
  BATCH_SIZE: 128
  LR:
    SCHEDULE: False
  LOG_INTERVAL: 50
  CHECKPOINT_INTERVAL: 500
  PATIENCE: 2500
  NUM_UPDATES: 20001
  MASK_RATIO: 0.25
  MASK_MODE: "timestep"

  TUNE_HP_JSON: './configs/sweep_generic.json'